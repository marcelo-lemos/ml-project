# STANDALONE = Starts MicroRTS as a standalone instance (No remote agents).
# GUI = Starts the microRTS GUI.
# SERVER = Starts MicroRTS as a server instance.
# CLIENT = Starts MicroRTS as a client instance.
launch_mode=STANDALONE

### NETWORKING ###
# Only needed if modes are SERVER/CLIENT
# server_address=127.0.0.1
# server_port=9898
# 1 = XML
# 2 = JSON
# serialization_type=2



#### microRTS settings ###

# MAP
map_location=maps/24x24/basesWorkers24x24.xml

# The max number of cycles the game will perform.
max_cycles=3000

# If false, the players have full vision of the map.
partially_observable=false

# Versions of the Unit Type Table (DEFAULT = 2)
# 1 = original
# 2 = original finetuned
# 3 = non-deterministic version of original finetuned (damages are random)
UTT_version=2

# Conflict policies (DEFAULT = 1)
# 1 = A conflict resolution policy where move conflicts cancel both moves
# 2 = A conflict resolution policy where move conflicts are solved randomly
# 3 = A conflict resolution policy where move conflicts are solved by alternating the units trying to move
conflict_policy=1

### RUNNER SETTINGS ###
# number of games to play
runner.num_games=100
runner.output=training/summary_self-vs-Light.txt
#runner.trace_prefix = training/trace

### STANDALONE Settings ###
# Only needed if mode is STANDALONE
# Set which AIs will play
AI1=metabot.MetaBot
AI2=ai.abstraction.LightRush

##### MetaBot settings ##### 
#uses this very config file for MetaBot
player1.config = config/selfplay/test-vs-Light.properties
#player2.config = config/selfplay/p2.train.properties

output.choices_prefix = training/choices_self-vs-Light

# the file to load weights from
rl.bin_input = training/vsSelf_b_500.weights

# specifies the portfolio members
portfolio.members = WorkerRush, LightRush, RangedRush, HeavyRush, Expand, BuildBarracks

### the parameters below are related to the reinforcement learning algorithm ###
# specifies the type of learning agent
rl.agent = "sarsa"

# the initial value of exploration rate
rl.epsilon.initial = 0.0

# epsilon is multiplied by this decay factor after each episode. 
# setting as 1 makes epsilon constant
rl.epsilon.decay = 1

# the initial value of learning rate
rl.alpha.initial = 0.0

# alpha is multiplied by this decay factor after each episode. 
# setting as 1 makes alpha constant
rl.alpha.decay = 1

# the discount factor
rl.gamma = 0.9

# the eligibility trace (not used yet)
rl.lambda = 0

# the feature extractor
rl.feature.extractor = quadrant_model

# the map is divided in quadrant_division x quadrant_division quadrants
# this parameter is specific of the quadrant_model
rl.feature.extractor.quadrant_division = 3

# the random seed (if not specified, it will load the default seed)
rl.random.seed = 1

# the prefix of the output file to save weights in binary format
#rl.output.binprefix = training/vsSelf_b

