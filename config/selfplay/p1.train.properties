#output.choices_prefix = training/choices

# specifies the portfolio members

portfolio.members = WorkerRush, LightRush, RangedRush, HeavyRush, Expand, BuildBarracks

### the parameters below are related to the reinforcement learning algorithm ###
# specifies the type of learning agent
rl.agent = "sarsa"

# the initial value of exploration rate
rl.epsilon.initial = 0.1

# epsilon is multiplied by this decay factor after each episode. 
# setting as 1 makes epsilon constant
rl.epsilon.decay = 1

# the initial value of learning rate
rl.alpha.initial = 0.1

# alpha is multiplied by this decay factor after each episode. 
# setting as 1 makes alpha constant
rl.alpha.decay = 1

# the discount factor
rl.gamma = 0.9

# the eligibility trace (not used yet)
rl.lambda = 0

# the feature extractor
rl.feature.extractor = quadrant_model

# the map is divided in quadrant_division x quadrant_division quadrants
# this parameter is specific of the quadrant_model
rl.feature.extractor.quadrant_division = 3

# the random seed (if not specified, it will load the default seed)
rl.random.seed = 1

# the prefix of the output file to save weights in binary format
rl.output.binprefix = training/vsSelf_b

# the prefix of the output file to save weights in human-readable format
#rl.output.humanprefix = training/vslight